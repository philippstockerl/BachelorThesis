# Evaluation and Comparison Conditions

To ensure a sound comparison between the robust (static) and dynamic (D* Lite) models, the following experimental conditions and procedures are established.

## 1. GRF (Map) Generation Conditions

The generation of Gaussian Random Field (GRF) maps, which serve as the basis for the simulation environment, follows these conditions:

- **Grid Setup**: The GRF is defined on a regular grid of size \( N \times N \), where \( N \) is fixed for all experiments to maintain consistency.

- **Kernel**: A squared exponential (Gaussian) kernel is used to define the covariance structure of the GRF:
  \[
  k(\mathbf{x}, \mathbf{x}') = \sigma^2 \exp\left(-\frac{\|\mathbf{x} - \mathbf{x}'\|^2}{2\ell^2}\right),
  \]
  where \(\sigma^2\) is the variance and \(\ell\) is the length scale.

- **Normalization**: The GRF samples are normalized to have zero mean and unit variance before further processing.

- **Variance**: The variance parameter \(\sigma^2\) is fixed across all generated maps to ensure comparability.

- **Blur**: A Gaussian blur with a fixed kernel size and standard deviation is applied to smooth the generated maps, simulating realistic spatial correlations.

- **Forecast–Actual Pairing**: For each GRF realization representing the "actual" environment, a corresponding "forecast" map is generated by adding controlled noise or perturbations, simulating forecast uncertainty.

- **Edge Cost Generation**: Edge costs for the path planning graph are derived from the forecast maps by transforming the GRF values into traversal costs. This transformation is consistent across all experiments.

- **Export Requirements**: All generated GRF maps (forecast and actual) are exported with the same colorbar normalization and range to allow direct visual comparison. The normalization range is fixed based on the global minimum and maximum across all maps.

## 2. Robust vs. Dynamic Comparison Conditions

The comparison of the robust (static) and dynamic (D* Lite) models is conducted under the following conditions:

- **Model Inputs**: Both models receive the same forecast maps as input. The robust model plans a static path before execution, while the dynamic model replans online based on observed actual costs.

- **Γ Scaling**: The robustness parameter \(\Gamma\) in the robust model is scaled relative to the forecast cost \(c\) as:
  \[
  \Gamma = \gamma \cdot \frac{c}{\bar{c}},
  \]
  where \(\gamma\) is a tuning parameter and \(\bar{c}\) is the mean forecast cost over the grid. This scaling ensures fairness by adjusting \(\Gamma\) proportionally to the cost scale.

- **Fairness**: To ensure a fair comparison, both models are evaluated on the same set of forecast–actual map pairs and under identical initial and goal conditions.

- **Evaluation Metrics**: The following metrics are recorded and compared:
  - **Path Cost**: Total traversal cost of the planned path.
  - **Path Length**: Number of edges or total distance traversed.
  - **Computation Time**: Time taken for planning and replanning.
  - **Coefficient of Variation (CV)**: Defined as
    \[
    \text{CV} = \frac{\text{standard deviation of costs}}{\text{mean cost}},
    \]
    calculated over multiple runs or scenarios to quantify cost variability.

- **Scenario Matrix**: Experiments are conducted over a matrix of scenarios varying:
  - Forecast uncertainty levels.
  - Robustness parameter \(\gamma\).
  - Starting and goal positions.
  
  This matrix allows systematic evaluation of model performance under diverse conditions.

- **Sanity Checks**:
  - Correlation between forecast and actual maps is logged for each scenario to assess forecast quality.
  - The same colorbar normalization is used for visualizations to allow direct comparison.
  - Results include both quantitative metrics and qualitative assessments (e.g., path visualizations).

These conditions ensure that the evaluation rigorously compares the robust and dynamic models under controlled, reproducible, and scientifically sound settings.
